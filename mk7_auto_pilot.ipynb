{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime \n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "#ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class ClassificationDataset:\n",
    "    def __init__(self, image_paths, targets, resize=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        \n",
    "        # ImageNet mean and std\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        self.aug = albumentations.Compose(\n",
    "            [\n",
    "                albumentations.Normalize(\n",
    "                    mean, std, max_pixel_value=255.0, always_apply=True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item]).convert(\"RGB\")\n",
    "        targets = self.targets[item]\n",
    "        accelerate_target = targets[2]\n",
    "        steering_target = targets[0]\n",
    "        #print(targets)\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (224, 224), resample=Image.BILINEAR\n",
    "            )\n",
    "\n",
    "        image = np.array(image)\n",
    "        augmented = self.aug(image=image)\n",
    "        image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        #accelerate_target = \n",
    "\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"accelerate_targets\": torch.tensor(accelerate_target, dtype=torch.int),\n",
    "            \"steering_targets\": torch.tensor(steering_target, dtype=torch.float)\n",
    "            #\"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "DATA_DIR = \"/home/gabriel/√Årea de Trabalho/mk7_dataset\"\n",
    "image_files = glob.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
    "targets_orig = [eval(x.split(\"/\")[-1][:-4].split(\"-hash_\")[0]) for x in image_files] #list of lists containing the relevant list in the filename for each image\n",
    "\n",
    "train_images,test_images,train_targets,test_targets = model_selection.train_test_split(\n",
    "    image_files, targets_orig, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "    image_paths=train_images,\n",
    "    targets=train_targets,\n",
    "    resize=(224,224)\n",
    ")\n",
    "# looks like the images are not entering properly and only their paths\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataset = ClassificationDataset(\n",
    "    image_paths=test_images,\n",
    "    targets=test_targets,\n",
    "    resize=(224, 224),\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[-1.4158, -1.4158, -1.4158,  ..., -0.9877, -0.8849, -0.8335],\n",
       "          [-1.4158, -1.4158, -1.4158,  ..., -0.9877, -0.9020, -0.8335],\n",
       "          [-1.4158, -1.4158, -1.4158,  ..., -0.9877, -0.9363, -0.9192],\n",
       "          ...,\n",
       "          [-0.7308, -0.7137, -0.5938,  ..., -0.3198, -0.4054, -0.5596],\n",
       "          [-0.6623, -0.5938, -0.4739,  ..., -0.2342, -0.1828, -0.3027],\n",
       "          [-0.5938, -0.5596, -0.4397,  ..., -0.1999, -0.0972, -0.0629]],\n",
       " \n",
       "         [[ 0.9405,  0.9405,  0.9405,  ...,  1.1506,  1.2206,  1.3081],\n",
       "          [ 0.9405,  0.9405,  0.9405,  ...,  1.1681,  1.2381,  1.3081],\n",
       "          [ 0.9405,  0.9405,  0.9405,  ...,  1.1681,  1.2206,  1.2731],\n",
       "          ...,\n",
       "          [-0.6176, -0.6001, -0.4776,  ..., -0.1975, -0.2675, -0.4251],\n",
       "          [-0.5301, -0.4601, -0.3550,  ..., -0.1099, -0.0574, -0.1800],\n",
       "          [-0.4601, -0.4076, -0.3200,  ..., -0.0574,  0.0476,  0.0651]],\n",
       " \n",
       "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "          ...,\n",
       "          [-0.5321, -0.5147, -0.3578,  ...,  0.0256, -0.0615, -0.2184],\n",
       "          [-0.4624, -0.3578, -0.1835,  ...,  0.1128,  0.1651,  0.0431],\n",
       "          [-0.3404, -0.2532, -0.1138,  ...,  0.1476,  0.2522,  0.2871]]]),\n",
       " 'accelerate_targets': tensor(1, dtype=torch.int32),\n",
       " 'steering_targets': tensor(0.0349)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)\n",
    "#TODO: the item is correct with targets, now it must be passed as a batch properly in the train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MK7AutoPilot(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = resnet18(pretrained=True)\n",
    "\n",
    "        # I think it needs 2 Linear layers, one for steering and one for accelerate, then backprop and update them individually\n",
    "        self.accelerate_fc = nn.Linear(1000, 1)\n",
    "        self.steering_fc = nn.Linear(1000, 1)\n",
    "        self.accelerate = nn.Sigmoid()\n",
    "        self.steering = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.backbone(inputs)\n",
    "        x_accelerate = self.accelerate_fc(x)\n",
    "        x_steering = self.steering_fc(x)\n",
    "        accelerate =  self.accelerate(x_accelerate)\n",
    "        steering = self.steering(x_steering)\n",
    "        \n",
    "        return accelerate, steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MK7AutoPilot()\n",
    "device = torch.device('cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "accelerate_loss = nn.BCELoss() # backprop sigmoid, accelerate in mk7 is just yes or no, no float value :( \n",
    "steering_loss = nn.HingeEmbeddingLoss() # backprop tanh (-1 to 1, float represeting the steering from left to right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3718]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.4749]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand((1, 3, 224, 224))\n",
    "acc_, steer_ = model(img)\n",
    "print(acc_)\n",
    "print(steer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches=1\n",
    "# for epoch in range(epoches):\n",
    "#     #train\n",
    "#     model.train()\n",
    "    \n",
    "# for inputs,labels in train_loader:\n",
    "#     print(labels)\n",
    "#         inputs = inputs.to(device)\n",
    "    #labels = labels.to(device)\n",
    "   # acc,steer=model(inputs)\n",
    "   # print(acc)\n",
    "    #print(steer)\n",
    "    #print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epoches):  # loop over the dataset multiple times\n",
    "#     print(f\"\\nepoch: {epoch} / {(epoches-1)}\")\n",
    "#     print(\"----------\")\n",
    "#     for phase in [\"train\",\"val\"]:\n",
    "#         if phase == \"train\":\n",
    "#             model.train()\n",
    "#             description=\"[bold gold3]Training...  [/bold gold3]\"\n",
    "#         else:\n",
    "#             model.eval()\n",
    "#             description=\"[bold blue_violet]Validating...[/bold blue_violet]\"\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "#         for inputs,labels in track(dataloaders[phase],description=description):\n",
    "#             # TODO: pass labels as the list of values, hardcode proper idx as steering and accelerate values\n",
    "#             # get the inputs; data is a list of [inputs, labels]\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             ## --> forward step\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             with torch.set_grad_enabled(phase == 'train'):\n",
    "                \n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds= torch.max(outputs,1)\n",
    "#                 loss = criterion(outputs,labels)# calculates loss its picks the right labels and calculate based on the output predicted\n",
    "                \n",
    "#                 # right way for this model i guess\n",
    "#                 #acc_out, steer_out = model(inputs)\n",
    "#                 #acc_loss = accelerate_loss(acc_out,acc_target)\n",
    "#                 #steer_loss = steering_loss(steer_out,steer_target) \n",
    "           \n",
    "#         # --> backward step\n",
    "#             if phase== \"train\":\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 # acc_loss.backward()\n",
    "#                 # steer_loss.backward()\n",
    "#                 # optimizer.step()\n",
    "\n",
    "#             ## --> statistics\n",
    "#             running_loss += running_loss * inputs.size(0) # batchsize\n",
    "#             running_corrects += torch.sum(preds==labels.data)# I think preds is a batch of 4 inputs, and we compare the sum of statements that matches the labels.data\n",
    "            \n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc =running_corrects.double()/ dataset_sizes[phase]\n",
    "            \n",
    "#         print(f\"[bold]Loss[/bold] : {loss:.4f}   [bold]Acc[/bold]: {epoch_acc:.4f}\")\n",
    "#         if phase==\"val\" and epoch_acc > best_acc:\n",
    "#             best_acc = epoch_acc\n",
    "#             best_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
